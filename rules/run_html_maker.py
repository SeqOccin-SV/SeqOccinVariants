#!/usr/bin/env python


import pandas as pd
from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt
import os
from pretty_html_table import build_table
import seaborn as sns
import re


# Parsing a file generated by samtools stats for a pbmm2 output bam.
def parse_stat_file(stat_file):
    rawdata = defaultdict(list)
    with open(stat_file) as fin:
        for line in fin:
            if line.startswith('#'):
                continue
            fields = line.rstrip().split("\t")
            rawdata[fields[0]].append("\t".join(fields[1:]))
    return rawdata


# get all SN data from the samtools stats file. This data is the global information
# such as the number of mapped reads or the average sequence length.
def parse_SN(data):
    parsed_data = defaultdict()
    for line in data:
        sections = line.split("\t")
        field = sections[0].strip()[:-1]
        field = field.replace(" ", "_")
        value = float(sections[1].strip())
        parsed_data[field] = value
    return parsed_data


# get all the COV data (coverage) from the samtools stats file. Input data
# is a dictionary built by parse_stat_file().
def parse_COV(data):
    values = []
    counts = []
    for line in data:
        fields = line.split("\t")
        values.append(int(fields[1]))
        counts.append(int(fields[2]))
    x = np.array(values)
    c = np.array(counts)
    #mean_depth = float(np.mean(np.dot(x,c)/np.sum(c)))
    return {'counts': c, 'values': x}



def parse_RL(data):
    length = []
    count = []
    for line in data:
        fields = line.split("\t")
        length.append(int(fields[0]))
        count.append(int(fields[1]))
    l = np.array(length)
    c = np.array(count)
    return {'lengths' : l, 'counts' : c}


# get the COV data from a samtools stats file and stores it in a DataFrame.
def cov_df(file):
    data = parse_stat_file(file)
    cov = parse_COV(data['COV'])
    df = pd.DataFrame(cov)
    return df


# build axes to draw a single barplot
def draw_ax(ax, df, columnX, columnY, title='Distribution', xlabel = 'x', ylabel = 'y'):
    ax.bar(df[columnX],df[columnY])
    ax.set_title(title, fontsize=18)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    

# Draw a single barplot
def draw_depth_distribution(ax, df, title = "Depth distribution") :
    draw_ax(ax, df[df['values']<100], 'values', 'counts', title, 'values', 'counts')


# Defines a title for plots based on file name.
def get_title(filename, ext = "") :
    filename = filename.split('/')[-1]
    CCS = re.compile(r"(.*)-(CCS.*)")
    CLR = re.compile(r"(.*)-(CLR.*)")
    ONT = re.compile(r"(.*)-(ONT.*)")
    mCCS = re.match(CCS, filename)
    mCLR = re.match(CLR, filename)
    mONT = re.match(ONT, filename)
    if mCCS is not None :
        return mCCS.group(1) + " " + ext
    if mCLR is not None :
        return mCLR.group(1) + " " + ext
    if mONT is not None :
        return mONT.group(1) + " " + ext
    return (filename.split('.')[0] + " " + ext)


# Add a individual (file) to the dictionary used to plot the depth distributions.
def add_in_dic(dic, file) :
    ax = 'ax' + str(len(dic['ax']) + 1)
    dic['ax'].append(ax)
    dic[ax] = (file, cov_df(file))


# Creat and compleat the dictionary used to plot the depth distributions.
def build_dic_depth_distribution(files) :
    dic = {'ax' : []}
    for f in files :
        add_in_dic(dic, f)
    return dic


# Draw and arrange a barplot for each input individual.
def build_png(files, png) :
    dic = build_dic_depth_distribution(files)
    tab = dic['ax']
    fig, (tab) = plt.subplots(len(dic['ax']),1, constrained_layout=True, 
          figsize=(12,3*len(dic['ax'])))
    for i in range(len(tab)) :
        title = get_title(dic[dic['ax'][i]][0], "depth distribution")
        draw_depth_distribution(tab[i], dic[dic['ax'][i]][1], title)
    plt.savefig(png)


# Creat a string containing the header for the html file.
def html_header() :
    return """ <!DOCTYPE html>
    <html>
    <head>
    <title>Page Title</title>
    </head>
    <body>
    <style>
        div {
            text-align: center;
            margin: 50px 50px
        }
        table.center {
            margin-left: auto;
            margin-right: auto;
        }
    </style>
    <div>
    """
   
    
# Creat a string containing the ending for the html file.
def html_tail() :
    return """</div>
    </body>
    </html> 
    """
    

# Write the html body between the header and the tail.
def write_html(file, body) :
    with open(file, 'w') as f :
        f.write(html_header())
        f.write(body)
        f.write(html_tail())


# Input grapth in html file using base64 text respresentation of a image.
# This allows the html file to be moved without caring about the png path.
def html_graph_output(png, title, text = "", subtext = "") :
    header = "<h2>" + title + "</h2>\n"
    info = "<p>" + text + "</p>\n"
#    img = '<img src="' + str(png) + '">' + "\n"
    img = '<img width="900" src="data:image/png;base64,' + png + '">\n'
    under_img = "<p>" + subtext + "</p>\n<br><br>"
    return header + info + img + under_img


def html_build_table(dic, title = "Structural variant stats", text = "", subtext = "") :
    body = "<h2>" + title + "</h2>\n"
    body += "<p>" + text + "</p>\n"
    df = pd.DataFrame(dic)
    table = build_table(df, 'blue_light', font_size = "25px")
    new_table = table[0:28] + 'center' + table[37:]
    body += new_table
    body += "<p>" + subtext + "</p>\n"
    body += "<br><br>"
    return body


# Parse sevral files containing extracted data from vcf files and stor it in a
# python dictionary.
# files [str] : list of files built from :
#    bcftools query -f "%ID\n" name_HiFi-CCS-pbmm2-pbsv.vcf.gz | 
#    cut -f2 -d'.' | sort | uniq -c > sv_stats.txt
def parse_vcf_for_table(files) :
    grouped = {'name' : [], 'BND' : [], 'DEL' : [], 'INS' : [], 'INV' : [], 'SPLIT' : []}
    indel = []
    for file in files :
        dic = {}
        indel.append(get_title(file))
        with open(file, 'r') as f :
            for line in f :
                l = line.split()
                if len(l) > 1 :
                    dic[l[1]] = l[0]
        for i in grouped.keys() :
            if i in dic.keys() :
                grouped[i].append(dic[i])
            else :
                grouped[i].append(0)
    grouped['name'] = indel
    return grouped


# all_files (str) : file containing a list of file names to use.
def get_files_from_file(all_files) :
    files = []
    with open(all_files, 'r') as f :
        for sub_files in f :
            try :
                with open(sub_files[:-1], 'r') :
                    files.append(sub_files[:-1])
            except IOError :
                print(sub_files + " not found or can't be read.")
    return files


# Takes a txt file (generated with base64) and returns a single string of it's
# content.
def file_to_single_string(file) :
    txt = ""
    with open(file) as f :
        for l in f :
            txt += l
    return txt


# Converts a png file a string that can be read by html as an imbeded image.
def png_to_base64txt(png, txt = None) :
    if txt is None :
        txt = '.'.join(png.split('.')[:-1]) + ".txt"
#    txt = png
    base64 = "base64 " + png + " > " + txt
    os.system(base64)
    return file_to_single_string(txt)


# Creat a color scaling depending on the number of individuals for plotting.
def color_scaling(column_length) :
    colors = [(0,0,1)]
    nbG = 0
    nbR = 0
    if column_length%2 != 0 :
        nbG = int(column_length/2)+1
        nbR = int(column_length/2)
    else :
        nbG = nbR = int(column_length/2)
    r = 0
    g = 0
    b = 1
    incr = 1/column_length
    for i in range(0,nbG) :
        r += incr
        b -= incr
        g += incr*2
        colors.append((r,g,b))
    for i in range(0,nbR) :
        r += incr
        b -= incr
        g -= incr*2
        colors.append((r,g,b))
    return colors


def hist_draw(df, ax, col) :
    ax.bar(df['name'], df[col], color = color_scaling(len(df['name'])))
    max_cap = int(max(df[col]) + max(df[col])/10)
    ax.set_ylim([0, max_cap])
    ax.set_title(col, fontsize=18)
    # ax.set_xticklabels(df['name'], rotation=45, ha='right')


def pbsv_graphs(dic, png = 'pbsv_stats.png') :
    df = pd.DataFrame(dic)
    data = ['BND', 'DEL', 'INS', 'INV', 'SPLIT']
    for d in data :
        df[d] = df[d].astype(int)    
    df['SV'] = df[['BND', 'DEL', 'INS', 'INV', 'SPLIT']].sum(axis = 1)
    data.append('SV')
    axs = ['ax1', 'ax2', 'ax3', 'ax4', 'ax5', 'ax6']
    fig, ((axs[0], axs[1]), (axs[2], axs[3]), (axs[4], axs[5])) = plt.subplots(3,2, constrained_layout=True, figsize=(12,6))
    for i in range(0, len(data)) :
        hist_draw(df, axs[i], data[i])
    # plt.tight_layout()
    plt.savefig(png)


# Also works for deepvariant stats files
def parse_longshot_for_table(files) :
    grouped = {'name' : [], 'SNPs' : [], 'transitions' : [], 'transvertions' : [], 'ts/tv' : []}
    indel = []
    for file in files :
        indel.append(file.split('/')[-1].split('-')[0])
        with open(file, 'r') as f :
            for line in f :
                l = line.split()
                if l[0] == 'TSTV' :
                    grouped['name'].append(file.split('/')[-1].split('-')[0])
                    grouped['SNPs'].append(int(l[2]) + int(l[3]))
                    grouped['transitions'].append(int(l[2]))
                    grouped['transvertions'].append(int(l[3]))
                    grouped['ts/tv'].append(float(l[4]))
    return grouped


def draw_variantsizes(file, ax, title = 'Insertion and Deletion size distribution') :
    df = pd.read_csv(file, sep='\t')
    df_INS = df[df['svtype'] == 'INS'][['svtype', 'size']]
    df_DEL = df[df['svtype'] == 'DEL'][['svtype', 'size']]

    df_INS['occurence'] = df_INS.groupby('size')['size'].transform('size')
    df_INS.drop_duplicates(inplace=True)
    df_INS.sort_values('size', inplace=True)

    df_DEL['occurence'] = df_DEL.groupby('size')['size'].transform('size')
    df_DEL.drop_duplicates(inplace=True)
    df_DEL.sort_values('size', inplace=True)

    # fig, ax = plt.subplots(1,1, constrained_layout=True, figsize=(12,3))
    df_INS.plot(x='size', y='occurence', ax=ax, label='INS')
    df_DEL.plot(x='size', y='occurence', ax=ax, label='DEL')
    ax.set_title(title, fontsize=18)
    ax.set_xlabel('size')
    ax.set_ylabel('count')
    ax.set_xscale('symlog')
    ax.legend()


def draw_variantsizes_png(files, png) :
    axs = []
    for i in range(len(files)) :
        axs.append("ax" + str(i))
    fig, axs = plt.subplots(len(files),1, constrained_layout=True, figsize=(12,3*len(files)))
    for i in range(len(files)) :
        draw_variantsizes(files[i], axs[i], title = get_title(files[i], "INS & DEL size distribution"))
    plt.savefig(png)


def png_naming(file_name) :
    return "stats/" + file_name.split('/')[-1] + ".png"


def write_full_html(bam_stats_files, pbsv_stats_files, variantsizes_files, snp_stats_files, html = "stats/pipeline_output.html") :
    sns.set_theme()
    body = "<h1>PacBio Variants pipeline stats</h1>\n<br><br>"

    # Build pbmm2 depth graphs
    bam = get_files_from_file(bam_stats_files) 
    png = png_naming(bam_stats_files)
    build_png(bam, png)
    txt = png_to_base64txt(png)
    body += html_graph_output(txt, 'Samples pbmm2')
    
    # get pbsv vcf file data for variant table.
    pbsv = get_files_from_file(pbsv_stats_files)
    dic = parse_vcf_for_table(pbsv)
    body += html_build_table(dic, 'name', 'pbsv structural variant stats')
    png = png_naming(pbsv_stats_files)
    pbsv_graphs(dic, png)
    txt = png_to_base64txt(png)
    body += html_graph_output(txt, 'pbsv stats barplots')

    # pbsv insertion & deletion size distribution :
    variantsizes = get_files_from_file(variantsizes_files)
    png = png_naming(variantsizes_files)
    draw_variantsizes_png(variantsizes, png)
    txt = png_to_base64txt(png)
    body += html_graph_output(txt, 'Insertion Deletion stats')

    # SNP with longshot or deepvariant :
    snp = get_files_from_file(snp_stats_files)
    dic_ls = parse_longshot_for_table(snp)
    if snp_stats_files.split('.')[-2] == "longshot" :
        body += html_build_table(dic_ls, 'name', 'longshot SNP stats')
    elif snp_stats_files.split('.')[-2] == "sv" :
        body += html_build_table(dic_ls, 'name', 'sv SNP stats')

    write_html(html, body)






